{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Adding `utils` directory to `PYTHONPATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../utils\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Reading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing load_csv function from read_data module\n",
    "from read_data import load_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>in the 22nd century, a paraplegic marine is di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>captain barbossa, long believed to be dead, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206647</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>a cryptic message from bond’s past sends him o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>following the death of district attorney harve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49529</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>john carter is a war-weary, former military ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                     title  \\\n",
       "0     19995                                    Avatar   \n",
       "1       285  Pirates of the Caribbean: At World's End   \n",
       "2    206647                                   Spectre   \n",
       "3     49026                     The Dark Knight Rises   \n",
       "4     49529                               John Carter   \n",
       "\n",
       "                                                tags  \n",
       "0  in the 22nd century, a paraplegic marine is di...  \n",
       "1  captain barbossa, long believed to be dead, ha...  \n",
       "2  a cryptic message from bond’s past sends him o...  \n",
       "3  following the death of district attorney harve...  \n",
       "4  john carter is a war-weary, former military ca...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading cleaned data\n",
    "cleaned_df = load_csv('clean_data', 'cleaned_data.csv')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "- It is the process of converting text into numbers, specifically into vectors so that a computer can understand and work with them.\n",
    "- Computers don't understand text directly, they understand numbers.\n",
    "- So, before we feed text to a machine learning model, we need to turn our words into numerical form.\n",
    "- Imagine trying to train a model to detect whether a message is positive or negative.\n",
    "- If you just give it the sentence : `\"I love this movie!\"`\n",
    "- The model doesn't know what \"love\" or \"movie\" means.\n",
    "- You need to translate those words into numbers.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Let's suppose we have three sentences which we are going to convert into numbers.\n",
    "- `I love NLP`\n",
    "- `I love deep learning`\n",
    "- `NLP is fun`\n",
    "\n",
    "#### Step 1 : Build a Vocabulary (Unique Words)\n",
    "- From all 3 sentences, list every unique word :\n",
    "\n",
    "| Word         |\n",
    "|--------------|\n",
    "| I            |\n",
    "| love         |\n",
    "| NLP          |\n",
    "| deep         |\n",
    "| learning     |\n",
    "| is           |\n",
    "| fun          |\n",
    "\n",
    "- So the vocabulary size is 7.\n",
    "\n",
    "#### Step 2 : Bag of Words (BoW)\n",
    "- Now we represent each sentence as a vector of word counts using the vocabulary above.\n",
    "\n",
    "| Sentence                 | I | love | NLP | deep | learning | is | fun |\n",
    "|--------------------------|---|------|-----|------|----------|----|-----|\n",
    "| I love NLP | 1 | 1 | 1   | 0    | 0        | 0  | 0   |\n",
    "| I love deep learning | 1 | 1    | 0   | 1    | 1        | 0  | 0   |\n",
    "| NLP is fun | 0 | 0    | 1   | 0    | 0        | 1  | 1   |\n",
    "\n",
    "- Each row = one sentence.\n",
    "- Each column = one word from the vocabulary.\n",
    "- Each cell = how many times that word appeared in that sentence.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Stop Words\n",
    "- Stop words are the common, frequently occurring words in a language that don't carry much meaning on their own.\n",
    "- They are often removed from text because :\n",
    "    - They don't help distinguish between documents/sentences.\n",
    "    - They add noise and increase vector size in models like Bag of Words (BoW).\n",
    "\n",
    "#### Example\n",
    "> `the`, `is`, `in`, `on`, `and`, `a`, `an`, `to`, `of`, `with`, `for`, `from`, `that`, `this`, `it`, etc.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### What is Stemming?\n",
    "- Stemming is the process of reducing a word to its base or root form (called the \"stem\").\n",
    "\n",
    "| Word | Stem |\n",
    "|:--:|:---:|\n",
    "| running | run |\n",
    "| runs | run |\n",
    "| runner | runner |\n",
    "| studied | studi |\n",
    "| studies |\tstudi |\n",
    "\n",
    "#### Problem with BoW without Stemming\n",
    "- BoW treats every unique word as different, even if they are grammatically related. Like :\n",
    "    - `\"He is running in the park.\"`\n",
    "    - `\"She runs every day.\"`\n",
    "- Without stemming, `running` ≠ `runs`.\n",
    "- So BoW thinks these are completely different words, even though they refer to the same root concept : \"`run`\".\n",
    "\n",
    "#### Benefit of Stemming\n",
    "- Stemming reduces related words to their common root form, like :\n",
    "    - `running`, `runs`, `ran` → `run`\n",
    "    - `studies`, `studying`, `studied` → `studi`\n",
    "- Now BoW will group them together!\n",
    "\n",
    "#### BoW Matrix (Without Stemming)\n",
    "- Vocabulary : `[\"he\", \"is\", \"running\", \"in\", \"the\", \"park\", \"she\", \"runs\", \"every\", \"day\"]`\n",
    "\n",
    "| Sentence          | he | is | running | in | the | park | she | runs | every | day |\n",
    "|-------------------|----|----|---------|----|-----|------|-----|------|-------|-----|\n",
    "| Sentence 1        | 1  | 1  | 1       | 1  | 1   | 1    | 0   | 0    | 0     | 0   |\n",
    "| Sentence 2        | 0  | 0  | 0       | 0  | 0   | 0    | 1   | 1    | 1     | 1   |\n",
    "\n",
    "- These vectors are very different even though the meaning is similar.\n",
    "\n",
    "#### BoW Matrix (With Stemming)\n",
    "- New Vocabulary : `[\"he\", \"is\", \"run\", \"in\", \"the\", \"park\", \"she\", \"every\", \"day\"]`\n",
    "\n",
    "| Sentence          | he | is | run | in | the | park | she | every | day |\n",
    "|-------------------|----|----|-----|----|-----|------|-----|-------|-----|\n",
    "| Sentence 1        | 1  | 1  | 1   | 1  | 1   | 1    | 0   | 0     | 0   |\n",
    "| Sentence 2        | 0  | 0  | 1   | 0  | 0   | 0    | 1   | 1     | 1   |\n",
    "\n",
    "- Now \"`running`\" and \"`runs`\" are treated the same as \"`run`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stem function from text_stemming module\n",
    "from text_stemming import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>in the 22nd century, a parapleg marin is dispa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>captain barbossa, long believ to be dead, ha c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206647</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>a cryptic messag from bond’ past send him on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>follow the death of district attorney harvey d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49529</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>john carter is a war-weary, former militari ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                     title  \\\n",
       "0     19995                                    Avatar   \n",
       "1       285  Pirates of the Caribbean: At World's End   \n",
       "2    206647                                   Spectre   \n",
       "3     49026                     The Dark Knight Rises   \n",
       "4     49529                               John Carter   \n",
       "\n",
       "                                                tags  \n",
       "0  in the 22nd century, a parapleg marin is dispa...  \n",
       "1  captain barbossa, long believ to be dead, ha c...  \n",
       "2  a cryptic messag from bond’ past send him on a...  \n",
       "3  follow the death of district attorney harvey d...  \n",
       "4  john carter is a war-weary, former militari ca...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying stemming on tags column\n",
    "cleaned_df['tags'] = cleaned_df['tags'].apply(stem)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Applying Bag of Words on `tags` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying BOW on tags column\n",
    "vectors = cv.fit_transform(cleaned_df['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4381, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of vectors array\n",
    "vectors.shape\n",
    "# Here 4381 rows are the tags for each movie\n",
    "# And, 5000 columns are the most common 5000 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '007', '10', ..., 'zone', 'zoo', 'zooeydeschanel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of unique words (Top 5000 most common words)\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'front', 'here', 'other', 'behind', 'forty', 'thereupon', 'us', 'will', 'have', 'interest', 'ours', 'them', 'thereby', 'became', 'sincere', 'con', 'even', 'every', 'do', 'whereby', 'yourselves', 'only', 'next', 'two', 'de', 'back', 'wherein', 'herself', 'elsewhere', 'take', 'thereafter', 'be', 'from', 'can', 'too', 'done', 'while', 'this', 'about', 'then', 'before', 'six', 'un', 'how', 'had', 'least', 'keep', 'enough', 'couldnt', 'i', 'with', 'to', 'your', 'along', 'together', 'wherever', 'or', 'fifty', 'sixty', 'a', 'each', 'because', 'etc', 'and', 'becomes', 'thence', 'both', 'herein', 'something', 'those', 'empty', 'formerly', 'ourselves', 'still', 'some', 'they', 'she', 'noone', 'cry', 'thus', 'amount', 'former', 'hasnt', 'anyway', 'under', 'made', 'whereupon', 'same', 'himself', 'please', 'are', 'afterwards', 'nowhere', 'amoungst', 'go', 'anyone', 'not', 'we', 'latter', 'whereafter', 'much', 'however', 'someone', 'third', 'becoming', 'eg', 'across', 'describe', 'toward', 'mine', 'than', 'again', 'less', 'several', 'eleven', 'except', 'these', 'last', 'none', 'no', 'system', 'twenty', 'put', 'upon', 'seeming', 'thick', 'was', 'yours', 'per', 'first', 'hundred', 'thru', 'most', 'if', 'his', 'of', 'well', 'therefore', 'nor', 'after', 'at', 'through', 'everywhere', 'meanwhile', 'anywhere', 'become', 'when', 'five', 'myself', 'whereas', 'whole', 'via', 'often', 'during', 'but', 'move', 'onto', 'you', 'eight', 'against', 'namely', 'nevertheless', 'mill', 'where', 'towards', 'mostly', 'nine', 'one', 'three', 'alone', 'find', 'whom', 'their', 'few', 'my', 'without', 'in', 'the', 'over', 'whose', 'throughout', 'name', 'somewhere', 'below', 'that', 'him', 'everyone', 'ie', 'may', 'me', 'yourself', 'its', 'fill', 'ltd', 'any', 'already', 'since', 'almost', 'otherwise', 'above', 'whether', 'such', 'many', 'by', 'themselves', 'between', 'anything', 'whence', 'twelve', 'nobody', 'somehow', 'whoever', 'is', 'rather', 'hereby', 'seemed', 'latterly', 'due', 'all', 'also', 'hence', 'part', 'as', 'sometime', 'should', 'which', 'within', 'around', 'off', 're', 'another', 'fire', 'see', 'never', 'he', 'perhaps', 'others', 'although', 'detail', 'either', 'until', 'now', 'why', 'been', 'more', 'on', 'for', 'else', 'beside', 'her', 'show', 'could', 'always', 'being', 'top', 'therein', 'who', 'neither', 'thin', 'full', 'were', 'ever', 'itself', 'up', 'ten', 'beyond', 'bill', 'nothing', 'call', 'give', 'bottom', 'whenever', 'cant', 'once', 'side', 'further', 'out', 'very', 'what', 'fifteen', 'among', 'four', 'has', 'into', 'would', 'yet', 'whatever', 'an', 'might', 'must', 'own', 'hereafter', 'anyhow', 'sometimes', 'our', 'serious', 'am', 'found', 'so', 'though', 'get', 'whither', 'down', 'there', 'co', 'seems', 'it', 'besides', 'seem', 'indeed', 'hereupon', 'everything', 'inc', 'moreover', 'cannot', 'beforehand', 'hers', 'amongst'})\n"
     ]
    }
   ],
   "source": [
    "# List of all stop words from tags column\n",
    "print(cv.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Calculating Cosine Similarity of Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "- In the context of Bag of Words (BoW), cosine similarity is a metric used to measure the similarity between two text vectors.\n",
    "- Since BoW transforms text into vectors, cosine similarity allows you to compare how similar two text are, based on their word distributions.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Why Cosine Similarity?\n",
    "- Cosine similarity is often preferred because it focuses on the angle between vectors rather than their magnitude.\n",
    "- This means it's more concerned with how the words are distributed in each document rather than how long or short the documents are.\n",
    "- It ranges between -1 and 1 :\n",
    "    - 1 means the vectors are identical (very similar).\n",
    "    - 0 means the vectors are orthogonal (completely dissimilar).\n",
    "    - -1 would indicate completely opposite vectors, though this is rare in typical document similarity scenarios.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Formula for Cosine Similarity\n",
    "Given two vectors, $A$ and $B$, the cosine similarity is calculated as :\n",
    "$$\\text{cosine similarity} = \\frac{A \\cdot B}{|A| \\cdot |B|}$$\n",
    "where:\n",
    "- $A \\cdot B$ is the dot product of the two vectors.\n",
    "- $|A|$ and $|B|$ are the euclidean norms (magnitude) of the vectors.\n",
    "\n",
    "<hr>\n",
    "\n",
    "#### Example\n",
    "- Let's say we have two documents and after applying Bag of Words, we get the following vectors (with word counts) :\n",
    "\n",
    "| Word  | Doc 1 | Doc 2 |\n",
    "|-------|-------|-------|\n",
    "| cat   | 2     | 1     |\n",
    "| dog   | 1     | 2     |\n",
    "\n",
    "- Now, let's compute the cosine similarity between `Doc 1` and `Doc 2` :\n",
    "\n",
    "#### 1. Dot product of `Doc 1` and `Doc 2`:\n",
    "$$(2 \\times 1) + (1 \\times 2) + (0 \\times 1) = 2 + 2 + 0 = 4$$\n",
    "\n",
    "#### 2. Magnitude of `Doc 1`:\n",
    "$$\\text{Doc 1} = \\sqrt{(2^2) + (1^2) + (0^2)} = \\sqrt{4 + 1} = \\sqrt{5}$$\n",
    "\n",
    "#### 3. Magnitude of `Doc 2`:\n",
    "$$\\text{Doc 2} = \\sqrt{(1^2) + (2^2) + (1^2)} = \\sqrt{1 + 4 + 1} = \\sqrt{6}$$\n",
    "\n",
    "#### 4. Cosine Similarity:\n",
    "$$\\text{cosine similarity} = \\frac{4}{\\sqrt{5} \\times \\sqrt{6}} \\approx \\frac{4}{\\sqrt{30}} \\approx 0.7303$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4381, 4381)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of similarity array\n",
    "# It contains similarity between one vector and every other vector \n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.08238526, 0.0836242 , ..., 0.01887128, 0.04484485,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity of 1st movie with every other movie\n",
    "similarity[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Exporting `similarity` as `pickle` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing export_as_pickle function from serialization module\n",
    "from serialization import export_as_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved as 'similarity.pkl' successfully.\n"
     ]
    }
   ],
   "source": [
    "# Exporting similarity as pickle file \n",
    "export_as_pickle(similarity, 'models', 'similarity.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
